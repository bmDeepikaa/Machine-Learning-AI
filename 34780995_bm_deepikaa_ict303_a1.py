# -*- coding: utf-8 -*-
"""34780995_BM_DEEPIKAA_ICT303_A1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cwB_vQSb2QWguADWbc1km3T7CIyh5_iE

#  **ICT303 - Assignment 1**

**Your name: B M DEEPIKAA**

**Student ID: 34780995**

**Email: deepikaa180301@gmail.com**

# Contents
#### MLP MODEL
#### VGG16 from scratch
#### VGG16 using the pretrained Pytorch

Import all the libraries needed
"""

# !tensorboard --logdir=runs --host=0.0.0.0 --port=6006
from google.colab import drive
drive.mount('/content/drive')
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
from torchvision import datasets, transforms
import time
import matplotlib.pyplot as plt
import numpy as np
import sys
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.metrics import average_precision_score
from sklearn.metrics import precision_recall_curve
from io import BytesIO
from PIL import Image
from torchvision import models

"""

*   load data into variables then into image folders under torchvision to be able to use it to train and test the accuracy of the data set.
*   load/mount the dataset to drive to access the data without needing to export to retirve the data making it faster

"""

# 1. Load the dataset
transform = transforms.Compose([
    transforms.Resize((32, 32)),  # Resize to 32x32
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])
#replace the datasets root path to your own when testing accordingly
# datasets are created into train/test/test datasets which are allocated based on the path in the drive to be used later on  during the training
train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/ICT303/a1/data/train', transform=transform)
valid_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/ICT303/a1/data/valid', transform=transform)
test_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/ICT303/a1/data/test', transform=transform)

# Create DataLoaders
batch_size = 10 #uses lesser memory
trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
validloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=1)
testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)

print("List of classes: ", train_dataset.classes, "\n")

# def log_scatter_plot(writer, all_labels, all_predictions, step):
#     plt.figure(figsize=(10, 7))

#     # Create scatter plot
#     plt.scatter(all_labels, all_predictions, alpha=0.5)
#     plt.xlabel('True Labels')
#     plt.ylabel('Predicted Labels')
#     plt.title('Scatter Plot')
#     plt.xlim(-0.6, max(all_labels)+0.6)
#     plt.ylim(-0.6, max(all_predictions)+0.6)

#     buf = BytesIO() #save in memory using buf object
#     plt.savefig(buf, format='png')
#     plt.close()
#     buf.seek(0)

#     # Load the image and convert
#     img = plt.imread(buf)
#     img = (img * 255).astype(np.uint8)  #  uint8
#     img = np.transpose(img, (2, 0, 1))  #switch from  height,width,channels to channels, height, width

#     # write image to TensorBoard
#     writer.add_image('Scatter Plot of True vs Predicted', img, step)

def log_confusion_matrix(writer, all_labels, all_predictions, step):
    cm = confusion_matrix(all_labels, all_predictions)

    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=train_dataset.classes,
                yticklabels=train_dataset.classes)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.title('Confusion Matrix')
    plt.tight_layout()

    buf = BytesIO() #Save into a IO object to save memeory
    plt.savefig(buf, format='png')
    plt.close()
    buf.seek(0)

    # Read image n store in tensorboard
    img = plt.imread(buf)
    img = np.transpose(img, (2, 0, 1))
    # write image to TensorBoard
    writer.add_image('Confusion Matrix', img, step)



def log_and_show_images(writer, model, data_loader, classes, step):
    model.eval()

    with torch.no_grad():
        dataiter = iter(data_loader)
        images, labels = next(dataiter)  #iterate through batches or pictures

        # predictions of the image using torch max
        outputs = model(images)
        predicted_labels = torch.max(outputs, 1).indices

        img_grid = torchvision.utils.make_grid(images)
        img_grid = img_grid / 2 + 0.5  # Unnormalize the images
        npimg = img_grid.numpy()

        plt.figure(figsize=(10, 7))
        plt.imshow(np.transpose(npimg, (1, 2, 0)))
        plt.axis('off')  # Hide axes since there is a no need, its just to display images no numbers involved
        plt.show()

        # Print out the expected and correct images classification to view
        print('Truth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(images.shape[0])))
        print('Predicted: ', ' '.join(f'{classes[predicted_labels[j]]:5s}' for j in range(images.shape[0])))

        # write the images and labels into tensorBoard in 2 tabs, Images and Text tabs as we used the summary writer
        writer.add_image('Images', img_grid, step)
        writer.add_text('Truth', ' '.join(f'{classes[labels[j]]:5s}' for j in range(images.shape[0])), step)
        writer.add_text('Predicted', ' '.join(f'{classes[predicted_labels[j]]:5s}' for j in range(images.shape[0])), step)

"""# MLP model


1.   The MLPtrainer class, which verifies and trains the training and validating data, I used PyTorch rather than Keras to create an MLP (multi-layered Percepton) model. First, make the image 32x32x3 by flattening the layers to 1-dimensional image.
2.   
The size is reduced in the first linear layer, the units are reduced from 64 to 32 in the second linear layer, and the output size of the classes is provided in the final layer. incorporates Adam's optimization techniques and loss.

3.
Next, To determine the best model, train the dataset using the model's training class, which contains the training dataset, validate the validation dataset, use epochs and learning rate of the optimiser to figure the best model.

4. compute the map's and the test and validation datasets' accuracy.The confusion matrix, which was used to determine how misclassified parts of the data in each model were, and some visualizations of the training process over the epochs were provided in TensorBoard, which is located in the.ipynb file, will also share all of these data vizualization (you won't need to open a different tab to view the graphs).

### MLP MODEL
"""

# 2. Define the MLP model
class MLP(nn.Module):
    def __init__(self, inputSize=32 * 32 * 3, outputSize=8, lr=1e-5):
        super(MLP, self).__init__()
        self.layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(inputSize, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, outputSize),
        )
        self.lr = lr

    def forward(self, X):
        return self.layers(X)

    def loss(self, y_hat, y):
        fn = nn.CrossEntropyLoss()
        return fn(y_hat, y)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), self.lr)

"""### Trainer Class for MLP"""

class mlpTrainer:
    def __init__(self, n_epochs=10): #initially set the constructor to run with 10 epoch if epoch not specified when used
        self.max_epochs = n_epochs
        self.train_losses = []
        self.train_accuracies = []
        self.valid_losses = []
        self.valid_accuracies = []
        self.writer = SummaryWriter('runs/mlp_board')


    def fit(self, model, train_data, valid_data):
        self.model = model
        self.optimizer = model.configure_optimizers()
        #run each epoch and use the fit_epoch and validate the repsective dataset such as train and valid
        for epoch in range(self.max_epochs):
            train_loss, train_accuracy = self.fit_epoch(train_data)
            valid_loss, valid_accuracy = self.validate(valid_data)
            #add to the list of the accuries and losses
            self.train_losses.append(train_loss)
            self.train_accuracies.append(train_accuracy)
            self.valid_losses.append(valid_loss)
            self.valid_accuracies.append(valid_accuracy)

            #  add to tensorboard for a graph/plot using scalar
            self.writer.add_scalar('training loss', train_loss, epoch)
            self.writer.add_scalar('training accuracy', train_accuracy, epoch)
            self.writer.add_scalar('validation loss', valid_loss, epoch)
            self.writer.add_scalar('validation accuracy', valid_accuracy, epoch)
            print(f'Epoch [{epoch + 1}/{self.max_epochs}], '
                  f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
                  f'Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}')

        print("Training process has finished")
        self.writer.close()  # Close the writer after training

# method catered to check the accuracy on the training dataset
    def fit_epoch(self, data):
        current_loss = 0.0
        correct = 0
        total = 0
        self.model.train()

        for inputs, target in data:
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.model.loss(outputs, target)
            loss.backward()
            self.optimizer.step()

            current_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

        epoch_loss = current_loss / len(data)
        epoch_accuracy = correct / total
        return epoch_loss, epoch_accuracy
# method catered to check the accuracy on the valid dataset
    def validate(self, data):
        current_loss = 0.0
        correct = 0
        total = 0
        self.model.eval()

        with torch.no_grad():
            for inputs, target in data:
                outputs = self.model(inputs)
                loss = self.model.loss(outputs, target)
                current_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()

        epoch_loss = current_loss / len(data)
        epoch_accuracy = correct / total
        return epoch_loss, epoch_accuracy

"""## Parameter tuning models

cases:

Epoch 50 has the best MLP model parameters


**EPOCH=30**

*   accuracy rate=85.60% , MAP :82.84% -valid dataset
*   accuracy rate=84.10% -test
*   Time taken : 18 mins



**EPOCH=50**

*   accuracy rate=88.48% , MAP :87.10%- valid datase
*   accuracy rate:86.92%
*   Time taken : 25 mins

### Version 1  -BEST MLP MODEL
**learning rate= 1e-4 &&
epoch =50**

number of epoch=50 as anything more on MLP seems to disconnect my drive that i have mounted to run the codes.

*   number of epoch=50 as anything more on MLP seems to disconnect my drive that i have mounted to run the codes.
*   the accuracy level has been the highest with this for both train/validation datasets.
*   matrix confusion shows that most of the classes where classified correctly mostly with less than 10 images in misclassifed classes in 3 occasions. Which proves a higher rate of accuracy in the models' prediction of the image classification
* Time taken for execution estimated to be ~25 mins
"""

# version 1 model parameters tuning

# 3. Create the MLP model
mlp_model = MLP(lr=1e-4)

# 4. Create the Trainer class instance with 50 epochs
trainer = mlpTrainer(n_epochs=50)

# 5. Train the model
trainer.fit(mlp_model, trainloader, validloader)

# Track training time
start_time = time.time()

# Get the training time duration results
end_time = time.time()
print("Training time: ", end_time - start_time)

"""### Version 2

**Learning rate =1e-4 && Epoch = 30**


*   The number of epoch allows the training to run for 30 loops which i assume is not enough to train the model as much as 50 epoch. Thus the models accuracy levels being lesser than 50.
*   Matrix confusion shows that there are high levels of misclassification of knitwears, more than the correct numbers there were more misclassifed. which was around 22 knitwears that was misclassied as tees.
*  Time taken for execution is estimated ~18mins


"""

# version 2 model parameters tuning

# # 3. Create the MLP model
# mlp_model = MLP(lr=1e-4)

# # 4. Create the Trainer class instance
# trainer = mlpTrainer(n_epochs=30)

# # 5. Train the model
# trainer.fit(mlp_model, trainloader, validloader)

# # Track training time
# start_time = time.time()

# # Get the training time duration results
# end_time = time.time()
# print("Training time: ", end_time - start_time)

"""### Evaluation"""

# 6. Evaluate the model on the validation set and test set
mlp_model.eval()
correctPrediction = 0
total = 0
mlpWriter=trainer.writer #intance of the mlp model writer

all_labels = []
all_scores = []

#VALIDATION ACCURACY
with torch.no_grad():
    for images, labels in validloader:
        outputs = mlp_model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correctPrediction += (predicted == labels).sum().item()

        # details calculated and added to all_labels /all_scoreds to formulate the MAP
        all_labels.extend(labels.numpy())
        all_scores.extend(torch.softmax(outputs, dim=1).numpy())
#basic calculation for accuracy
accuracy = correctPrediction / total
print(f'Validation Accuracy: {accuracy:.4f}')

# TEST ACCURACY
with torch.no_grad():
    for images, labels in testloader:
        outputs = mlp_model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correctPrediction += (predicted == labels).sum().item()

        # details calculated and added to all_labels /all_scores to formulate the MAP
        all_labels.extend(labels.numpy())
        all_scores.extend(torch.softmax(outputs, dim=1).numpy())

accuracy = correctPrediction / total
print(f'test Accuracy: {accuracy:.4f}')
mlpWriter.add_scalar('Test Accuracy', accuracy)

all_labels = np.array(all_labels)
all_scores = np.array(all_scores)

mlpWriter.close()

# formulate MAP for validation using lib
map_score = average_precision_score(all_labels, all_scores, average='macro')
print(f'Mean Average Precision (MAP): {map_score:.4f}')

"""### Plots/reports and TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
#classes of the pictures in dataset
classes = ('accessories', 'jackets', 'jeans', 'knitwear', 'shirts', 'shoes', 'shorts', 'tees')

mlpWriter=trainer.writer #intance of the mlp model writer
################# Confusion Matrix ###################

all_labels = []
all_predictions = []

with torch.no_grad():
    for images, labels in validloader:
        outputs = mlp_model(images)
        _, predicted = torch.max(outputs.data, 1)
        all_labels.extend(labels.numpy())
        all_predictions.extend(predicted.numpy())

# ############## TENSORBOARD ########################

log_and_show_images(mlpWriter, mlp_model, trainloader, classes, step=0)
# log_scatter_plot(mlpWriter, all_labels, all_predictions, step=0)
log_confusion_matrix(mlpWriter, all_labels, all_predictions, step=0)


# # Close the writer
mlpWriter.close()

# Launch TensorBoard
# %load_ext tensorboard
# %tensorboard --logdir runs/mlp_board

"""#VGG16 from scratch

### VGG16 model


1.   Create a customized VGG16 model by implementing the code in PyTorch rather than Keras.
2.   consists of manually building five layer blocks that follow the CNN model. There are two layers with activation in each layer. The dense layers, which include dropouts, come last.

3.
Next, To determine the best model, train the dataset using the model's training class, which contains the training dataset, validate the validation dataset, use epochs and learning rate of the optimiser to figure the best model.

4. compute the map's and the test and validation datasets' accuracy.The confusion matrix, which was used to determine how misclassified parts of the data in each model were, and some visualizations of the training process over the epochs were provided in TensorBoard, which is located in the.ipynb file, will also share all of these data vizualization (you won't need to open a different tab to view the graphs).
"""

class  CreateVGG16(nn.Module):
    def __init__(self, num_classes=8):
        super(CreateVGG16, self).__init__()

        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.fc_layers = nn.Sequential(
            nn.Linear(512 * 1 * 1, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc_layers(x)
        return x

    def loss(self, prediction, target):
        criterion = nn.CrossEntropyLoss()
        return criterion(prediction, target)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)
        return optimizer

"""### VGG16 Trainer"""

class CreateVGG16Trainer:
    def __init__(self, n_epochs=10):  #initially set the constructor to run with 10 epoch if epoch not specified when used
        self.max_epochs = n_epochs
        self.train_losses = []
        self.train_accuracies = []
        self.valid_losses = []
        self.valid_accuracies = []
        self.writer = SummaryWriter('runs/createVGG16')

    def fit(self, model, train_data, valid_data):
        self.model = model
        self.optimizer = model.configure_optimizers()
        #run each epoch and use the fit_epoch and validate the repsective dataset such as train and valid
        for epoch in range(self.max_epochs):
            train_loss, train_accuracy = self.fit_epoch(train_data)
            valid_loss, valid_accuracy = self.validate(valid_data)
            #add to the list of the accuries and losses
            self.train_losses.append(train_loss)
            self.train_accuracies.append(train_accuracy)
            self.valid_losses.append(valid_loss)
            self.valid_accuracies.append(valid_accuracy)

            #  add to tensorboard for graph/plot using scaler
            self.writer.add_scalar('Training Loss', train_loss, epoch)
            self.writer.add_scalar('Training Accuracy', train_accuracy, epoch)
            self.writer.add_scalar('Validation Loss', valid_loss, epoch)
            self.writer.add_scalar('Validation Accuracy', valid_accuracy, epoch)

            print(f'Epoch [{epoch + 1}/{self.max_epochs}], '
                  f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
                  f'Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}')

        print("Training process has finished")
        self.writer.close()

# method catered to check the accuracy on the training dataset
    def fit_epoch(self, data):
        current_loss = 0.0
        correct = 0
        total = 0
        self.model.train()

        for inputs, target in data:
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.model.loss(outputs, target)
            loss.backward()
            self.optimizer.step()

            current_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

        epoch_loss = current_loss / len(data)
        epoch_accuracy = correct / total
        return epoch_loss, epoch_accuracy
# method catered to check the accuracy on the valid dataset

    def validate(self, data):
        current_loss = 0.0
        correct = 0
        total = 0
        self.model.eval()

        with torch.no_grad():
            for inputs, target in data:
                outputs = self.model(inputs)
                loss = self.model.loss(outputs, target)
                current_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()

        epoch_loss = current_loss / len(data)
        epoch_accuracy = correct / total
        return epoch_loss, epoch_accuracy

"""## Parameter tuning models

Learning rate=1e-4 and epoch 30 is the Best model for create new  VGG16

cases:

**EPOCH=10**

*   accuracy rate=85.60% , MAP :77.75% -validation
*   accuracy rate: 84.10% -test
*   Time taken : 1h 3 min




**EPOCH=30** -Best VGG16 model

*   accuracy rate=90.31% , MAP :87.27% -valid dataset
*   accuracy rate=88.72% -test

*   Time taken: 2h 45 min

#### Model 1

**10 epochs**



*  The time taken for the model to run with 10 epochs is estimated ~1h3 mins which is long for 10 epochs
*   The accuracy rate is fairly lower compared  30 epoch.10 is 85% which is lower than 30's 90%.

*  the matrix showed that the rate mismatched details are high when epoch is in 10 which could be due to less epochs thus not being trained much to allow model to be smart enough to point out the correct class
"""

newVGG16_model = CreateVGG16(num_classes=8)
newVGG16trainer = CreateVGG16Trainer(n_epochs=10)

start_time = time.time()
newVGG16trainer.fit(newVGG16_model, trainloader, validloader)
end_time = time.time()

# Track and print training time
training_time = end_time - start_time
print("Training time: ", training_time)

"""#### Model 2 - Best Custom VGG16 model

**30 epochs**

*   The time taken for the model to run with 10 epochs is estimated ~2h45 mins
*   The accuracy rate is high at 90%.

*  the matrix showed that the rate mismatched details are low when epoch is 30 , however higher the epoch the better the chance of good training. in this case.

*  The machine was disconnecting from mount google drive every time the epoch hit 34 in this cycle thus had to cut off at 30.And is the best from testing
"""

newVGG16_model = CreateVGG16(num_classes=8)
newVGG16trainer = CreateVGG16Trainer(n_epochs=30)

start_time = time.time()
newVGG16trainer.fit(newVGG16_model, trainloader, validloader)
end_time = time.time()

# Track and print training time
training_time = end_time - start_time
print("Training time: ", training_time)

"""### Evaluate"""

# 6. Evaluate the model on the validation set
newVGG16_model.eval()
correctPrediction = 0
total = 0
VGG16Writer=newVGG16trainer.writer

all_labels = []
all_scores = []

#VALIDATION ACCURACY
with torch.no_grad():
    for images, labels in validloader:
        outputs = newVGG16_model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correctPrediction += (predicted == labels).sum().item()

        # details calculated and added to all_labels /all_scoreds to formulate the MAP
        all_labels.extend(labels.numpy())
        all_scores.extend(torch.softmax(outputs, dim=1).numpy())

accuracy = correctPrediction / total
print(f'Validation Accuracy: {accuracy:.4f}')

# TEST ACCURACY
with torch.no_grad():
    for images, labels in testloader:
        outputs = newVGG16_model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correctPrediction += (predicted == labels).sum().item()

        # details calculated and added to all_labels /all_scoreds to formulate the MAP
        all_labels.extend(labels.numpy())
        all_scores.extend(torch.softmax(outputs, dim=1).numpy())
#basic calculation for accuracy
accuracy = correctPrediction / total
print(f'test Accuracy: {accuracy:.4f}')
VGG16Writer.add_scalar('Test Accuracy', accuracy)
VGG16Writer.close()

# switch to numpy arrays for easy usage when calculating map
all_labels = np.array(all_labels)
all_scores = np.array(all_scores)
# formulate MAP for validation using lib
map_score = average_precision_score(all_labels, all_scores, average='macro')
print(f'Mean Average Precision (MAP): {map_score:.4f}')

"""#### Classification images /TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.

#classes of the pictures in datasets
classes = ('accessories', 'jackets', 'jeans', 'knitwear', 'shirts', 'shoes', 'shorts', 'tees')

VGG16Writer=newVGG16trainer.writer

################# Confusion Matrix ###################

all_labels = []
all_predictions = []

with torch.no_grad():
    for images, labels in validloader:
        outputs = newVGG16_model(images)
        _, predicted = torch.max(outputs.data, 1)
        all_labels.extend(labels.numpy())
        all_predictions.extend(predicted.numpy())

# ############## TENSORBOARD ########################

log_and_show_images(VGG16Writer, newVGG16_model, trainloader, classes, step=0)
# log_scatter_plot(VGG16Writer, all_labels, all_predictions, step=0)
log_confusion_matrix(VGG16Writer, all_labels, all_predictions, step=0)

# Close the writer
VGG16Writer.close()


# Launch TensorBoard
# %load_ext tensorboard
# %tensorboard --logdir runs/createVGG16

"""# Pre Trained VGG16


1.   The preTrainedVGG16 class, which verifies and trains the training and validating data, I used PyTorch rather than Keras to use the built in VGG16 model.
2.   It follows the same logic as the model above this custom made VGG16 however its built and ready to use.It consists of manually building five layer blocks that follow the CNN model. There are two layers with activation in each layer. The dense layers, which include dropouts, come last.

3.
Next, To determine the best model, train the dataset using the model's training class, which contains the training dataset, validate the validation dataset, use epochs and learning rate of the optimiser to figure the best model.

4.  compute the map's and the test and validation datasets' accuracy.The confusion matrix, which was used to determine how misclassified parts of the data in each model were, and some visualizations of the training process over the epochs were provided in TensorBoard, which is located in the.ipynb file, will also share all of these data vizualization (you won't need to open a different tab to view the graphs).

### Pre-Trained Model Pytorch
"""

class PretrainedVGG16(nn.Module):
    def __init__(self, num_classes=8):
        super(PretrainedVGG16, self).__init__()
        self.vgg16 = models.vgg16(pretrained=True)
        # making sure that the last layer is connected to the total number of classes in the dataset we have
        self.vgg16.classifier[6] = nn.Linear(4096, num_classes)

    def forward(self, x):
        return self.vgg16(x)

    def loss(self, prediction, target):
        criterion = nn.CrossEntropyLoss()
        return criterion(prediction, target)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)
        return optimizer

"""#### Trainer class"""

from torch.utils.tensorboard import SummaryWriter

class PretrainedVGG16Trainer:
    def __init__(self, n_epochs=10):  #initially set the constructor to run with 10 epoch if epoch not specified when used
        self.max_epochs = n_epochs
        self.writer = SummaryWriter('runs/preTrainedVGG16')

    def fit(self, model, train_loader, valid_loader):
        self.model = model
        self.optimizer = model.configure_optimizers()
        #run each epoch and use the fit_epoch and validate the repsective dataset such as train and valid
        for epoch in range(self.max_epochs):
            train_loss, train_accuracy = self.fit_epoch(train_loader)
            valid_loss, valid_accuracy = self.validate(valid_loader)

            #  add to tensorboard for graph/plot using scaler
            self.writer.add_scalar('Training Loss', train_loss, epoch)
            self.writer.add_scalar('Training Accuracy', train_accuracy, epoch)
            self.writer.add_scalar('Validation Loss', valid_loss, epoch)
            self.writer.add_scalar('Validation Accuracy', valid_accuracy, epoch)

            print(f'Epoch [{epoch + 1}/{self.max_epochs}], '
                  f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
                  f'Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}')

        print("Training process has finished")
        self.writer.close()

# method catered to check the accuracy on the training dataset
    def fit_epoch(self, data_loader):
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0

        for inputs, targets in data_loader:
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.model.loss(outputs, targets)
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()

        epoch_loss = total_loss / len(data_loader)
        epoch_accuracy = correct / total
        return epoch_loss, epoch_accuracy
# method catered to check the accuracy on the valid dataset
    def validate(self, data_loader):
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, targets in data_loader:
                outputs = self.model(inputs)
                loss = self.model.loss(outputs, targets)
                total_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()

        epoch_loss = total_loss / len(data_loader)
        epoch_accuracy = correct / total
        return epoch_loss, epoch_accuracy

"""## Parameter tuning models


cases:

epoch=10

*   accuracy rate=91.10% , MAP :88.93% -validation
*   accuracy rate: 89.49% -test
*  Time taken : 2h34 min


epoch=25

*   accuracy rate=94.76% , MAP :93.00% -valid dataset
*   accuracy rate=93.08% -test

* Time Taken : 11h 5 min

### Model 1

**epoch =10**



*   Time taken for execution is estimated around ~2h34 min
*   the accuracy rate is lower than one of higher epoch due to under training.
"""

VGG16model = PretrainedVGG16(num_classes=8)
VGG16trainer = PretrainedVGG16Trainer(n_epochs=10)


start_time = time.time()
VGG16trainer.fit(VGG16model, trainloader, validloader)
end_time = time.time()

# Track and print training time
training_time = end_time - start_time
print("Training time: ", training_time)

"""##Model 2
**EPOCH =25**



*   the total time taken for execution was around ~11hrs which was long to train the data
*   the reason why i could not run more than 25 was becuase of the duration it took to run during fitting.
*  this model had the highest fitting and properly classified data according to matrix
* according to accuracy as well the highest accuracy range of 94%


"""

VGG16model = PretrainedVGG16(num_classes=8)
VGG16trainer = PretrainedVGG16Trainer(n_epochs=25)


start_time = time.time()
VGG16trainer.fit(VGG16model, trainloader, validloader)
end_time = time.time()

# Track and print training time
training_time = end_time - start_time
print("Training time: ", training_time)

"""## EVALUATE"""

# 6. Evaluate the model on the validation set
VGG16model.eval()
correctPrediction = 0
total = 0
preTrainedVGG16Writer=VGG16trainer.writer
all_labels = []
all_scores = []

#VALIDATION ACCURACY
with torch.no_grad():
    for images, labels in validloader:
        outputs = VGG16model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correctPrediction += (predicted == labels).sum().item()

        # details calculated and added to all_labels /all_scoreds to formulate the MAP
        all_labels.extend(labels.numpy())
        all_scores.extend(torch.softmax(outputs, dim=1).numpy())

accuracy = correctPrediction / total
print(f'Validation Accuracy: {accuracy:.4f}')

# TEST ACCURACY
with torch.no_grad():
    for images, labels in testloader:
        outputs = VGG16model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correctPrediction += (predicted == labels).sum().item()

        # details calculated and added to all_labels /all_scoreds to formulate the MAP
        all_labels.extend(labels.numpy())
        all_scores.extend(torch.softmax(outputs, dim=1).numpy())
#basic calculation for accuracy
accuracy = correctPrediction / total
print(f'test Accuracy: {accuracy:.4f}')
preTrainedVGG16Writer.add_scalar('Test Accuracy', accuracy)
preTrainedVGG16Writer.close()
# switch to numpy arrays for easy usage when calculating map
all_labels = np.array(all_labels)
all_scores = np.array(all_scores)

# formulate MAP for validation using lib
map_score = average_precision_score(all_labels, all_scores, average='macro')
print(f'Mean Average Precision (MAP): {map_score:.4f}')

"""## PLOTS/ TENSORBOARD"""

# Commented out IPython magic to ensure Python compatibility.

#classes of the pictures in datasets

classes = ('accessories', 'jackets', 'jeans', 'knitwear', 'shirts', 'shoes', 'shorts', 'tees')

preTrainedVGG16Writer=VGG16trainer.writer

################# Confusion Matrix ###################

all_labels = []
all_predictions = []

with torch.no_grad():
    for images, labels in validloader:
        outputs = VGG16model(images)
        _, predicted = torch.max(outputs.data, 1)
        all_labels.extend(labels.numpy())
        all_predictions.extend(predicted.numpy())

# ############## TENSORBOARD ########################

log_and_show_images(preTrainedVGG16Writer, VGG16model, trainloader, classes, step=0)
# log_scatter_plot(VGG16Writer, all_labels, all_predictions, step=0)
log_confusion_matrix(preTrainedVGG16Writer, all_labels, all_predictions, step=0)

# Close the writer
preTrainedVGG16Writer.close()


# Launch TensorBoard
# %load_ext tensorboard
# %tensorboard --logdir runs/preTrainedVGG16

"""# Conclusion


* If time is not an issue **VGG16 MODEL 2** might be the best model to try as it is a a built in VGG16 model which allows less to no errors when training however i assume the training might either be overfitted or underfitted due to the number of images that we have to test with. The number of epochs being only 25 however run time being 11hrs might be the only concern, as its having the best accuracy levels in training,validation and test datasets.Its MAP level is also the highest. According to matrix the model seem to have very little misclassified classes out of the whole models tested.

* **The model could be improved further/ run faster more if the person running the code had a faster way to run their code such as running it locally instead of on google drive,The reason i could not test for more epochs was due to slow network and disconnecting of the google mounted drive to the project every few hours, However i believe the model can be made better with proper network**


This the csv format of the classified data(there is a order in the excel file):

For the full format of the file the excel sheet can be opened that was submitted with this file to see the order, best being order of model=1 .
```
model name,validation accuracy,test Accuracy,Learning rate,MAP,epochs,time taken for execution,order of model
VGG16 MODEL 2,94.76,93.08,1.00E-04,93%,25,11h5min,1,
VGG16 MODEL 1,91.1,89.49,1.00E-04,88.93%,10,2h34min,2
Custom VGG16 model 2,90.31,88.72,1.00E-04,87.27%,30,2h45min,3
MLP model 2,88.48,86.92,1.00E-04,87.10%,50,25min,4
MLP model 1,85.6,84.1,1.00E-04,82.84%,30,18min,5
Custom VGG16 model 1,85.6,84.1,1.00E-04,77.75%,10,1h 3 min,6
```

"""